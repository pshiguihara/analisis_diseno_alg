{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y Diseño de Algoritmos: Heaps en Top-K Ranking\n",
    "\n",
    "**Autor:** Pedro Shiguihara\n",
    "\n",
    "**Fecha:** 2 de febrero de 2026\n",
    "\n",
    "---\n",
    "\n",
    "Este notebook contiene la implementación completa del sistema de recomendación\n",
    "Top-K basado en MinHeap, su versión naive, y el benchmark comparativo.\n",
    "Está diseñado para ejecutarse en **Google Colab** haciendo streaming del dataset\n",
    "[McAuley-Lab/Amazon-Reviews-2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023)\n",
    "directamente desde HuggingFace, sin necesidad de descargar los archivos a disco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de dependencias\n",
    "\n",
    "Se instala `requests` (para streaming HTTP) y `matplotlib` (para el scatterplot).\n",
    "Ambas suelen estar disponibles en Colab por defecto, pero se asegura su presencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MinHeap — Implementación CLRS con indexación 0-based\n",
    "\n",
    "Estructura de datos pura que almacena tuplas `(score, item_id)` y mantiene\n",
    "el elemento con menor score en la raíz. Es la pieza central de `SistemaRec1`\n",
    "para seleccionar los Top-K productos en O(n log K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinHeap:\n",
    "    \"\"\"Min-heap que almacena tuplas (score, item_id).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data: list = []\n",
    "\n",
    "    @staticmethod\n",
    "    def parent(i: int) -> int:\n",
    "        return (i - 1) // 2\n",
    "\n",
    "    @staticmethod\n",
    "    def left(i: int) -> int:\n",
    "        return 2 * i + 1\n",
    "\n",
    "    @staticmethod\n",
    "    def right(i: int) -> int:\n",
    "        return 2 * i + 2\n",
    "\n",
    "    def min_heapify(self, i: int) -> None:\n",
    "        \"\"\"Corrige el sub-árbol con raíz en i (recursivo, CLRS).\"\"\"\n",
    "        n = len(self._data)\n",
    "        smallest = i\n",
    "        l = self.left(i)\n",
    "        r = self.right(i)\n",
    "        if l < n and self._data[l] < self._data[smallest]:\n",
    "            smallest = l\n",
    "        if r < n and self._data[r] < self._data[smallest]:\n",
    "            smallest = r\n",
    "        if smallest != i:\n",
    "            self._data[i], self._data[smallest] = self._data[smallest], self._data[i]\n",
    "            self.min_heapify(smallest)\n",
    "\n",
    "    def build_min_heap(self, array: list) -> None:\n",
    "        \"\"\"Construye el heap in-place a partir de array en O(n).\"\"\"\n",
    "        self._data = list(array)\n",
    "        for i in range(len(self._data) // 2 - 1, -1, -1):\n",
    "            self.min_heapify(i)\n",
    "\n",
    "    def heap_minimum(self):\n",
    "        if not self._data:\n",
    "            raise IndexError(\"heap_minimum en heap vacío\")\n",
    "        return self._data[0]\n",
    "\n",
    "    def heap_extract_min(self):\n",
    "        if not self._data:\n",
    "            raise IndexError(\"heap_extract_min en heap vacío\")\n",
    "        minimum = self._data[0]\n",
    "        self._data[0] = self._data[-1]\n",
    "        self._data.pop()\n",
    "        if self._data:\n",
    "            self.min_heapify(0)\n",
    "        return minimum\n",
    "\n",
    "    def heap_decrease_key(self, i: int, key) -> None:\n",
    "        if key > self._data[i]:\n",
    "            raise ValueError(\"La nueva clave es mayor que la clave actual\")\n",
    "        self._data[i] = key\n",
    "        while i > 0 and self._data[self.parent(i)] > self._data[i]:\n",
    "            p = self.parent(i)\n",
    "            self._data[i], self._data[p] = self._data[p], self._data[i]\n",
    "            i = p\n",
    "\n",
    "    def min_heap_insert(self, key) -> None:\n",
    "        self._data.append((float(\"inf\"), \"\"))\n",
    "        self.heap_decrease_key(len(self._data) - 1, key)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"MinHeap({self._data})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Función de score\n",
    "\n",
    "Combina calidad (rating promedio) con popularidad (cantidad de reviews):\n",
    "\n",
    "$$\\text{score} = \\overline{\\text{rating}} \\times \\ln(1 + N_{\\text{reviews}})$$\n",
    "\n",
    "Se define como función independiente para que ambos sistemas la compartan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(sum_ratings: float, count: int) -> float:\n",
    "    \"\"\"Calcula score = mean_rating * log(1 + N_reviews).\"\"\"\n",
    "    mean_rating = sum_ratings / count\n",
    "    return mean_rating * math.log(1 + count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming del dataset desde HuggingFace\n",
    "\n",
    "Lee el archivo JSONL de una categoría directamente desde HuggingFace\n",
    "vía HTTP streaming, sin guardarlo en disco. Cada línea se parsea y\n",
    "descarta inmediatamente, manteniendo en memoria solo el diccionario\n",
    "de agregación `{parent_asin: [sum_ratings, count]}`.\n",
    "\n",
    "Esto permite ejecutar el notebook en Google Colab sin necesidad de\n",
    "espacio en disco para archivos de hasta 31 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_BASE_URL = (\n",
    "    \"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023\"\n",
    "    \"/resolve/main/raw/review_categories\"\n",
    ")\n",
    "\n",
    "\n",
    "def stream_and_aggregate(category: str) -> dict:\n",
    "    \"\"\"Hace streaming del JSONL desde HuggingFace y agrega ratings.\n",
    "\n",
    "    Retorna dict[parent_asin] -> [sum_ratings, count].\n",
    "    \"\"\"\n",
    "    url = f\"{HF_BASE_URL}/{category}.jsonl\"\n",
    "    print(f\"  Streaming: {url}\")\n",
    "\n",
    "    aggregated: dict[str, list] = {}\n",
    "    n_reviews = 0\n",
    "\n",
    "    with requests.get(url, stream=True, timeout=300) as r:\n",
    "        r.raise_for_status()\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            review = json.loads(line)\n",
    "            parent_asin = review[\"parent_asin\"]\n",
    "            rating = review[\"rating\"]\n",
    "            if parent_asin in aggregated:\n",
    "                aggregated[parent_asin][0] += rating\n",
    "                aggregated[parent_asin][1] += 1\n",
    "            else:\n",
    "                aggregated[parent_asin] = [rating, 1]\n",
    "            n_reviews += 1\n",
    "\n",
    "    print(f\"  {n_reviews:,} reviews -> {len(aggregated):,} productos únicos\")\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SistemaRec1 — Top-K con MinHeap O(n log K)\n",
    "\n",
    "Mantiene un MinHeap de tamaño fijo K. Para cada producto:\n",
    "- Si el heap tiene menos de K elementos: insertar.\n",
    "- Si el score supera al mínimo del heap: extraer mínimo e insertar.\n",
    "\n",
    "Al final, extrae todos los elementos y los invierte para orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SistemaRec1:\n",
    "    \"\"\"Top-K con MinHeap — O(n log K).\"\"\"\n",
    "\n",
    "    def __init__(self, k: int = 10):\n",
    "        self.k = k\n",
    "\n",
    "    def top_k(self, aggregated: dict) -> list[tuple[float, str]]:\n",
    "        heap = MinHeap()\n",
    "        for parent_asin, (sum_ratings, count) in aggregated.items():\n",
    "            score = compute_score(sum_ratings, count)\n",
    "            if len(heap) < self.k:\n",
    "                heap.min_heap_insert((score, parent_asin))\n",
    "            elif score > heap.heap_minimum()[0]:\n",
    "                heap.heap_extract_min()\n",
    "                heap.min_heap_insert((score, parent_asin))\n",
    "        result = []\n",
    "        while len(heap) > 0:\n",
    "            result.append(heap.heap_extract_min())\n",
    "        result.reverse()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SistemaRecNaive — Top-K con sort completo O(n log n)\n",
    "\n",
    "Calcula el score de todos los n productos, los ordena con `sorted()`\n",
    "en O(n log n) y toma los primeros K. Es el enfoque directo pero\n",
    "asintóticamente peor que SistemaRec1 cuando K << n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SistemaRecNaive:\n",
    "    \"\"\"Top-K con sort completo — O(n log n).\"\"\"\n",
    "\n",
    "    def __init__(self, k: int = 10):\n",
    "        self.k = k\n",
    "\n",
    "    def top_k(self, aggregated: dict) -> list[tuple[float, str]]:\n",
    "        scored = [\n",
    "            (compute_score(sum_r, count), parent_asin)\n",
    "            for parent_asin, (sum_r, count) in aggregated.items()\n",
    "        ]\n",
    "        scored.sort(reverse=True)\n",
    "        return scored[: self.k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Métricas de sistemas de recomendación\n",
    "\n",
    "Se definen las métricas para comparar los rankings producidos por ambos sistemas:\n",
    "\n",
    "| Métrica | Propósito |\n",
    "|---|---|\n",
    "| **Precision@K** | Fracción de items en común entre ambos top-K |\n",
    "| **AP@K** | Penaliza items relevantes en posiciones tardías |\n",
    "| **NDCG@K** | Calidad de ranking con descuento logarítmico |\n",
    "| **Jaccard@K** | Solapamiento global de conjuntos |\n",
    "| **Spearman ρ** | Correlación de orden entre rankings |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ref: list, evl: list) -> float:\n",
    "    set_ref = {item_id for _, item_id in ref}\n",
    "    set_evl = {item_id for _, item_id in evl}\n",
    "    if not set_evl:\n",
    "        return 0.0\n",
    "    return len(set_ref & set_evl) / len(set_evl)\n",
    "\n",
    "\n",
    "def average_precision_at_k(ref: list, evl: list) -> float:\n",
    "    set_ref = {item_id for _, item_id in ref}\n",
    "    if not set_ref:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    sum_precision = 0.0\n",
    "    for i, (_, item_id) in enumerate(evl, 1):\n",
    "        if item_id in set_ref:\n",
    "            hits += 1\n",
    "            sum_precision += hits / i\n",
    "    return sum_precision / min(len(ref), len(evl)) if evl else 0.0\n",
    "\n",
    "\n",
    "def _dcg(gains: list[float]) -> float:\n",
    "    return sum(g / math.log2(i + 2) for i, g in enumerate(gains))\n",
    "\n",
    "\n",
    "def ndcg_at_k(ref: list, evl: list) -> float:\n",
    "    relevance = {item_id: score for score, item_id in ref}\n",
    "    eval_gains = [relevance.get(item_id, 0.0) for _, item_id in evl]\n",
    "    actual_dcg = _dcg(eval_gains)\n",
    "    ideal_gains = sorted(relevance.values(), reverse=True)\n",
    "    ideal_dcg = _dcg(ideal_gains)\n",
    "    if ideal_dcg == 0:\n",
    "        return 0.0\n",
    "    return actual_dcg / ideal_dcg\n",
    "\n",
    "\n",
    "def jaccard_at_k(ranking_a: list, ranking_b: list) -> float:\n",
    "    set_a = {item_id for _, item_id in ranking_a}\n",
    "    set_b = {item_id for _, item_id in ranking_b}\n",
    "    union = set_a | set_b\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return len(set_a & set_b) / len(union)\n",
    "\n",
    "\n",
    "def spearman_rho(ranking_a: list, ranking_b: list) -> float:\n",
    "    rank_a = {item_id: i for i, (_, item_id) in enumerate(ranking_a)}\n",
    "    rank_b = {item_id: i for i, (_, item_id) in enumerate(ranking_b)}\n",
    "    common = set(rank_a) & set(rank_b)\n",
    "    n = len(common)\n",
    "    if n < 2:\n",
    "        return float(\"nan\")\n",
    "    d_sq_sum = sum((rank_a[item] - rank_b[item]) ** 2 for item in common)\n",
    "    return 1 - (6 * d_sq_sum) / (n * (n ** 2 - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuración del benchmark\n",
    "\n",
    "Se definen las categorías con archivos JSONL menores a 2 GB y los\n",
    "valores de K a evaluar (`range(5, 1000, 100)`).\n",
    "\n",
    "El streaming se realiza una sola vez por categoría. Luego, la fase\n",
    "de ranking se ejecuta para cada valor de K, midiendo únicamente el\n",
    "costo algorítmico (sin I/O) para aislar la diferencia entre\n",
    "O(n log K) y O(n log n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_UNDER_2GB = [\n",
    "    \"Subscription_Boxes\",       # 8.95 MB\n",
    "    \"Magazine_Subscriptions\",   # 33.3 MB\n",
    "    \"Gift_Cards\",               # 50.2 MB\n",
    "    \"Digital_Music\",            # 78.8 MB\n",
    "    \"Health_and_Personal_Care\", # 227 MB\n",
    "    \"Handmade_Products\",        # 289 MB\n",
    "    \"All_Beauty\",               # 326.6 MB\n",
    "    \"Appliances\",               # 929.5 MB\n",
    "    \"Amazon_Fashion\",           # 1.05 GB\n",
    "    \"Musical_Instruments\",      # 1.56 GB\n",
    "    \"Software\",                 # 1.87 GB\n",
    "]\n",
    "\n",
    "K_VALUES = list(range(5, 1000, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Streaming y agregación de todas las categorías\n",
    "\n",
    "Se hace streaming de cada categoría una sola vez y se almacena el\n",
    "diccionario agregado en memoria. Esto evita re-descargar los datos\n",
    "en cada iteración del benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = {}\n",
    "\n",
    "for cat in CATEGORIES_UNDER_2GB:\n",
    "    print(f\"\\n[Streaming] {cat}\")\n",
    "    try:\n",
    "        aggregated_data[cat] = stream_and_aggregate(cat)\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] {e}\")\n",
    "\n",
    "print(f\"\\nCategorías cargadas: {len(aggregated_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Benchmark completo: variación de K\n",
    "\n",
    "Para cada categoría cargada, se ejecutan ambos sistemas con cada valor\n",
    "de K en `range(5, 1000, 100)`, midiendo únicamente el tiempo de la\n",
    "fase de ranking (sin I/O). Se calculan también las métricas de\n",
    "recomendación usando `SistemaRecNaive` como referencia.\n",
    "\n",
    "Los resultados se almacenan en una lista para generar el scatterplot\n",
    "y el reporte final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = []\n",
    "\n",
    "for cat, aggregated in aggregated_data.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Categoría: {cat} ({len(aggregated):,} productos)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        rec1 = SistemaRec1(k=k)\n",
    "        naive = SistemaRecNaive(k=k)\n",
    "\n",
    "        # Tiempo SistemaRec1\n",
    "        t0 = time.perf_counter()\n",
    "        results_rec1 = rec1.top_k(aggregated)\n",
    "        time_rec1_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        # Tiempo SistemaRecNaive\n",
    "        t0 = time.perf_counter()\n",
    "        results_naive = naive.top_k(aggregated)\n",
    "        time_naive_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        # Métricas\n",
    "        prec = precision_at_k(results_naive, results_rec1)\n",
    "        ap = average_precision_at_k(results_naive, results_rec1)\n",
    "        ndcg = ndcg_at_k(results_naive, results_rec1)\n",
    "        jacc = jaccard_at_k(results_rec1, results_naive)\n",
    "        rho = spearman_rho(results_naive, results_rec1)\n",
    "\n",
    "        row = {\n",
    "            \"category\": cat,\n",
    "            \"k\": k,\n",
    "            \"n_products\": len(aggregated),\n",
    "            \"time_rec1_ms\": round(time_rec1_ms, 4),\n",
    "            \"time_naive_ms\": round(time_naive_ms, 4),\n",
    "            \"precision_at_k\": round(prec, 4),\n",
    "            \"ap_at_k\": round(ap, 4),\n",
    "            \"ndcg_at_k\": round(ndcg, 4),\n",
    "            \"jaccard_at_k\": round(jacc, 4),\n",
    "            \"spearman_rho\": round(rho, 4) if not math.isnan(rho) else \"NaN\",\n",
    "        }\n",
    "        benchmark_results.append(row)\n",
    "\n",
    "        print(f\"  K={k:>4}  Rec1: {time_rec1_ms:>8.2f} ms  \"\n",
    "              f\"Naive: {time_naive_ms:>8.2f} ms  \"\n",
    "              f\"P@K={prec:.2f}  NDCG={ndcg:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal de mediciones: {len(benchmark_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Reporte CSV\n",
    "\n",
    "Se genera un CSV con todas las mediciones para su análisis posterior.\n",
    "En Google Colab, el archivo queda disponible en el panel de archivos\n",
    "de la izquierda para descargarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "CSV_PATH = \"benchmark_completo_colab.csv\"\n",
    "CSV_COLUMNS = [\n",
    "    \"category\", \"k\", \"n_products\",\n",
    "    \"time_rec1_ms\", \"time_naive_ms\",\n",
    "    \"precision_at_k\", \"ap_at_k\", \"ndcg_at_k\",\n",
    "    \"jaccard_at_k\", \"spearman_rho\",\n",
    "]\n",
    "\n",
    "with open(CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(benchmark_results)\n",
    "\n",
    "print(f\"CSV guardado en: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Scatterplot: Tiempo de ejecución vs Top-K\n",
    "\n",
    "Se genera un scatterplot por cada categoría evaluada. El eje X\n",
    "representa el valor de K y el eje Y el tiempo de ejecución en\n",
    "milisegundos. Dado que el streaming se realizó previamente, estos\n",
    "tiempos reflejan **únicamente la fase de ranking**, lo que permite\n",
    "observar la diferencia algorítmica real entre O(n log K) y O(n log n)\n",
    "sin el ruido del I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_in_results = list(dict.fromkeys(\n",
    "    row[\"category\"] for row in benchmark_results\n",
    "))\n",
    "\n",
    "for cat in categories_in_results:\n",
    "    rows_cat = [r for r in benchmark_results if r[\"category\"] == cat]\n",
    "    ks = [r[\"k\"] for r in rows_cat]\n",
    "    t_rec1 = [r[\"time_rec1_ms\"] for r in rows_cat]\n",
    "    t_naive = [r[\"time_naive_ms\"] for r in rows_cat]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.scatter(ks, t_rec1, label=\"SistemaRec1 (MinHeap)\", marker=\"o\")\n",
    "    ax.scatter(ks, t_naive, label=\"SistemaRecNaive (sort)\", marker=\"s\")\n",
    "    ax.set_xlabel(\"top-k\")\n",
    "    ax.set_ylabel(\"Tiempo de ejecución (ms)\")\n",
    "    ax.set_title(f\"SistemaRec1 vs SistemaRecNaive — {cat}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    png_path = f\"benchmark_{cat}.png\"\n",
    "    fig.savefig(png_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Guardado: {png_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Tabla resumen de métricas por categoría (K=505)\n",
    "\n",
    "Se muestra una tabla resumen con las métricas de recomendación\n",
    "para el valor central de K como referencia rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_mid = K_VALUES[len(K_VALUES) // 2]\n",
    "\n",
    "print(f\"{'Categoría':<30} {'P@K':>6} {'AP@K':>6} {'NDCG':>7} \"\n",
    "      f\"{'Jacc':>6} {'Spear':>7} {'Rec1(ms)':>10} {'Naive(ms)':>10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for row in benchmark_results:\n",
    "    if row[\"k\"] == k_mid:\n",
    "        print(f\"{row['category']:<30} \"\n",
    "              f\"{row['precision_at_k']:>6.2f} \"\n",
    "              f\"{row['ap_at_k']:>6.2f} \"\n",
    "              f\"{row['ndcg_at_k']:>7.4f} \"\n",
    "              f\"{row['jaccard_at_k']:>6.2f} \"\n",
    "              f\"{str(row['spearman_rho']):>7} \"\n",
    "              f\"{row['time_rec1_ms']:>10.2f} \"\n",
    "              f\"{row['time_naive_ms']:>10.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
